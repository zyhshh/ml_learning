【任务2 - 逻辑回归算法梳理】时长：2天 (截止时间2019-05-14晚10点)
1、逻辑回归与线性回归的联系与区别 
2、 逻辑回归的原理 
3、逻辑回归损失函数推导及优化 
4、 正则化与模型评估指标 
5、逻辑回归的优缺点 
6、样本不均衡问题解决办法
sklearn参数
学习时长：两天
参考：
经典的机器学习入门视频：[机器学习（Machine Learning）- 吴恩达（Andrew Ng）]
https://www.bilibili.com/video/av9912938?from=search&seid=5516998205005060196
Datawhale的著名开源项目：[南瓜书]
https://github.com/datawhalechina/pumpkin-book/tree/master/docs
西瓜书
李航统计学习
谷歌搜索
https://shimo.im/docs/F0i9zh4OdgIoRPHC/ 《初级算法打卡表》，可复制链接后用石墨文档 App 或小程序打开

1、逻辑回归与线性回归的联系与区别 
逻辑回归的模型 是一个非线性模型，sigmoid函数，又称逻辑回归函数。但是它本质上又是一个线性回归模型，因为除去sigmoid映射函数关系，其他的步骤，算法都是线性回归的。可以说，逻辑回归，都是以线性回归为理论支持的。
2、 逻辑回归的原理 
逻辑回归的核心思想是：如果回归的结果输出是一个连续值，而值的范围是无法限定的，那么想办法把这个连续结果值映射为可以帮助我们判断的结果值，从而进行分类。所以，从本质上讲，逻辑回归是在回归的基础上，进行了特殊的改进，而被用于分类问题上。

sigmoid函数连续，光滑，严格单调，以(0,0.5)中心对称，是一个非常良好的阈值函数。

当x趋近负无穷时，y趋近于0；趋近于正无穷时，y趋近于1；x=0时，y=0.5。当然，在x超出[-6,6]的范围后，函数值基本上没有变化，值非常接近，在应用中一般不考虑。

Sigmoid函数的值域范围限制在(0,1)之间，我们知道[0,1]与概率值的范围是相对应的，这样sigmoid函数就能与一个概率分布联系起来了。

Sigmoid函数的导数是其本身的函数，即f′(x)=f(x)(1−f(x))f′(x)=f(x)(1−f(x))，计算非常方便，也非常节省计算时间。推导过程如下： 
根据常用的求导公式，得到： 
https://img-blog.csdn.net/20170226222316999?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2FsdHJpdmVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast

3、逻辑回归损失函数推导及优化 
在模型确定后，需要用一个损失函数（loss function）或代价函数（cost function）来度量预测错误的程度。常用的损失函数有以下几种：
1）0-1损失函数： 
L(Y,f(X))={1,Y≠f(X)0,Y=f(X)
L(Y,f(X))={1,Y≠f(X)0,Y=f(X)

2）平方损失函数: 
L(Y,f(X))=(Y−f(X))2
L(Y,f(X))=(Y−f(X))2

3）绝对损失函数: 
L(Y,f(X))=|Y−f(X)|
L(Y,f(X))=|Y−f(X)|

4）对数损失函数或对数似然损失函数： 
L(Y,P(Y|X))=−logP(Y|X)
对于逻辑回归模型，使用的是对数损失函数作为代价函数

4、 正则化与模型评估指标 
逻辑回归也会面临过拟合问题，所以我们也要考虑正则化。常见的有L1正则化和L2正则化。
    逻辑回归的L1正则化的损失函数表达式如下，相比普通的逻辑回归损失函数，增加了L1的范数做作为惩罚，超参数α
α作为惩罚系数，调节惩罚项的大小。
    二元逻辑回归的L1正则化损失函数表达式如下：

图片: https://uploader.shimo.im/f/HuiXfNA37e0GIITu.png


其中||θ||1为θθ的L1范数。
    逻辑回归的L1正则化损失函数的优化方法常用的有坐标轴下降法和最小角回归法。
 
    二元逻辑回归的L2正则化损失函数表达式如下：
图片: https://uploader.shimo.im/f/uFuTvwX9nbE96w03.png
其中||θ||2为θθ的L2范数。
    逻辑回归的L2正则化损失函数的优化方法和普通的逻辑回归类似。

5、逻辑回归的优缺点 

优点：
1）预测结果是界于0和1之间的概率；
2）可以适用于连续性和类别性自变量；
3）容易使用和解释；

缺点：
1）对模型中自变量多重共线性较为敏感，例如两个高度相关自变量同时放入模型，可能导致较弱的一个自变量回归符号不符合预期，符号被扭转。​需要利用因子分析或者变量聚类分析等手段来选择代表性的自变量，以减少候选变量之间的相关性；
2）预测结果呈“S”型，因此从log(odds)向概率转化的过程是非线性的，在两端随着​log(odds)值的变化，概率变化很小，边际值太小，slope太小，而中间概率的变化很大，很敏感。 导致很多区间的变量变化对目标概率的影响没有区分度，无法确定阀值。
 
6、样本不均衡问题解决办法：


1.样本的过采样和欠采样。
2..使用多个分类器进行分类。
3.将二分类问题转换成其他问题。
4.改变正负类别样本在模型中的权重。

