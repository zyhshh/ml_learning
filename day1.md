[TOC]


## 【任务1 - 线性回归算法梳理】时长：2天
机器学习的一些概念
1. 有监督、无监督、泛化能力、过拟合欠拟合(方差和偏差以及各自解决办法)、交叉验证
2. 线性回归的原理
3. 线性回归损失函数、代价函数、目标函数
4. 优化方法(梯度下降法、牛顿法、拟牛顿法等)
5. 线性回归的评估指标
6. sklearn参数详解

### 有监督、无监督、泛化能力、过拟合欠拟合(方差和偏差以及各自解决办法)、交叉验证
- 监督学习，无监督学习
#### 监督学习
机器学习如何解决分类问题, 它的主要任务是将实例数据划分到合适的分类中。机器学习的另一项任务是回归,它主要用于预测数值型数据。大多数人可能都见过回归的例子——数据拟合曲线:通过给定数据点的最优拟合曲线。分类和回归属于监督学习,之所以称之为监督学习,是因为这类算法必须知道预测什么,即目标变量的分类信息。 
监督学习的常用算法：

|  |  |
| --- | --- |
| K近邻算法 | 线性回归 |
| 朴素贝叶斯算法 | 局部加权线性回归 |
| 支持向量机 | Ridge回归 |
| 决策树 | Lasso最小回归系数估计 |



#### 无监督学习
无监督学习,此时数据没有类别信息,也不会给定目标值。在无监督学习中, 将数据集合分成由类似的对象组成的多个类的过程被称为聚类; 将寻找描述数据统计值的过程称之为密度估计。此外,无监督学习还可以减少数据特征的维度,以便我们可以使用二维或三维图形更加直观地展示数据信息。

|  |  |
| --- | --- |
|K-均值  | 最大期望算法  |
| DBSCAN | Parzen窗设计 |


#### 二者不同点
1. 有监督学习方法必须要有训练集与测试样本。在训练集中找规律，而对测试样本使用这种规律。而非监督学习没有训练集，只有一组数据，在该组数据集内寻找规律。

2.    有监督学习的方法就是识别事物，识别的结果表现在给待识别数据加上了标签。因此训练样本集必须由带标签的样本组成。而非监督学习方法只有要分析的数据集的本身，预先没有什么标签。如果发现数据集呈现某种聚集性，则可按自然的聚集性分类，但不予以某种预先分类标签对上号为目的。

3. 非监督学习方法在寻找数据集中的规律性，这种规律性并不一定要达到划分数据集的目的，也就是说不一定要“分类”。
这一点是比有监督学习方法的用途要广。    譬如分析一堆数据的主分量，或分析数据集有什么特点都可以归于非监督学习方法的范畴。

4. 用非监督学习方法分析数据集的主分量与用K-L变换计算数据集的主分量又有区别。后者从方法上讲不是学习方法。因此用K-L变换找主分量不属于无监督学习方法，即方法上不是。而通过学习逐渐找到规律性这体现了学习方法这一点。在人工神经元网络中寻找主分量的方法属于无监督学习方法。 

#### 何时采用哪种方法
简单的方法就是从定义入手，有训练样本则考虑采用监督学习方法；无训练样本，则一定不能用监督学习方法。但是，现实问题中，即使没有训练样本，我们也能够凭借自己的双眼，从待分类的数据中，人工标注一些样本，并把它们作为训练样本，这样的话，可以把条件改善，用监督学习方法来做。对于不同的场景，正负样本的分布如果会存在偏移（可能大的偏移，可能比较小），这样的话，监督学习的效果可能就不如用非监督学习了

#### 泛化能力
学习方法的泛化能力(generalization ability)是指由该方法学习到的模型对未知数据的预测能力,是学习方法本质上重要的性质。现实中采用最多的办法是通过测试误差来评价学习方法的泛化能力。但这种评价是依赖于测试数据集的。因为测试数据集是有限的, 很有可能由此得到的评价结果是不可靠的。统计学习理论试图从理论上对学习方法的泛化能力进行分析。

泛化能力：是指一个模型应用到新样本的能力。这里的新样本是指没有出现在训练集中的数据。


过拟合欠拟合(方差和偏差以及各自解决办法)、交叉验证
线性回归的原理
线性回归损失函数、代价函数、目标函数
优化方法(梯度下降法、牛顿法、拟牛顿法等)
线性回归的评估指标
sklearn参数详解

过拟合欠拟合(方差和偏差以及各自解决办法)、交叉验证
1.过拟合：就是训练时的结果很好，但是在预测时结果不好的情况。

2.产生过拟合的原因：

（1)   模型的复杂度太高。比如：网络太深，

（2）过多的变量（特征）

（3）训练数据非常少。

3.如何解决过拟合？

避免过拟合的方法有很多：
（1）尽量减少特征的数量、
（2）early stopping、
（3）数据集扩增、
（4）dropout、
（5）正则化包括L1、L2、
（6）清洗数据

什么是欠拟合+为什么会产生欠拟合？（高偏差）+怎么解决欠拟合？

1.什么是欠拟合？

      模型没有很好地捕捉到数据特征，不能够很好地拟合数据的情况，就是欠拟合。

2.为什么会产生欠拟合？

因为模型不够复杂而无法捕捉数据基本关系，导致模型错误的表示数据。
比如：（1）如果对像是按照颜色和形状进行分类的，但是模型只能按照颜色来区分对象和将对象分类，因而一直会错误的分类对象。（2）我们的模型可能是多项式的形式，但是训练出来的模型却只能表示线性关系。

3.怎么解决欠拟合？

1）添加其他特征项，有时候我们模型出现欠拟合的时候是因为特征项不够导致的，可以添加其他特征项来很好地解决。例如，“组合”、“泛化”、“相关性”三类特征是特征添加的重要手段，无论在什么场景，都可以照葫芦画瓢，总会得到意想不到的效果。除上面的特征之外，“上下文特征”、“平台特征”等等，都可以作为特征添加的首选项。

2）添加多项式特征，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。例如上面的图片的例子。

3）减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。

避免欠拟合(刻画不够)
寻找更好的特征—–具有代表性的
用更多的特征—–增大输入向量的维度

方差、偏差的应用场景？
用于计算模型的好坏。具体是使用error公式。
Error = Bias^2 + Variance+Noise
什么是Bias(偏差)：Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，即算法本身的拟合能力
什么是Variance(方差)：Variance反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。反应预测的波动情况。
什么是Noise(噪声)：这就简单了，就不是你想要的真正数据，你可以想象为来破坏你实验的元凶和造成你可能过拟合的原因之一，至于为什么是过拟合的原因，因为模型过度追求Low Bias会导致训练过度，对测试集判断表现优秀，导致噪声点也被拟合进去了


三、什么是方差？

1.方差的定义：

什么是Variance(方差)：Variance反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。反应预测的波动情况。

2.方差和偏差的形象化表示？靶心和射击的结果。

https://img-blog.csdn.net/20180406141219967?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTIxOTc3NDk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70

其中，bias表示的是偏差，描述的是模型和预测结果和真实结果的差距;variance表示的是方差。

图中的靶心就是我们的真实值。

离靶心的距离反映了我们的偏差有多大。离靶心越近，偏差越小;离靶心越远，方差越大。

点的聚集程度反映了我们的方差有多大。越分散，方差越大。越聚拢，方差越小。

举个例子来理解：两个射击选手在射靶。甲射出的子弹很集中在某个区域，但是都偏离了靶心。我们说他的射击很稳定，但是不够准，准确性差。也就是说他的方差小（子弹很集中在某个区域），但是他的偏差大（子弹打中的地方距离靶心远）。相反，乙射出的子弹比较分散，但是有些很准，中了靶心。我们说他射击比较准，但是发挥不够稳定，稳定性差。 

所以，偏差是描述了准确性。方差是描述稳定性。

四、什么是偏差？

1.偏差的定义：

什么是Bias(偏差)：Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，即算法本身的拟合能力


方差、偏差和过拟合、欠拟合之间的关系?偏差、方差与欠拟合、过拟合之间又有什么关系呢？
过拟合会出现高方差问题
欠拟合会出现高偏差问题

https://img-blog.csdn.net/20170807200653074?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl3ZWliaW4xOTk0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast


1 交叉验证简介
1.1 交叉验证是什么
交叉验证的基本思想是把在某种意义下将原始数据(dataset)进行分组,一部分做为训练集(train set),另一部分做为验证集(validation set or test set),首先用训练集对分类器进行训练,再利用验证集来测试训练得到的模型(model),以此来做为评价分类器的性能指标。---来自百科
1.2 为什么需要交叉验证
假设有个未知模型具有一个或多个待定的参数，且有一个数据集能够反映该模型的特征属性（训练集）。
适应的过程是对模型的参数进行调整，以使模型尽可能反映训练集的特征。
如果从同一个训练样本中选择独立的样本作为验证集合，当模型因训练集过小或参数不合适而产生过拟合时，验证集的测试予以反映。
总的来说：交叉验证是一种预测模型拟合性能的方法。


线性回归原理
线性回归（Linear Regression）是一种通过属性的线性组合来进行预测的线性模型，其目的是找到一条直线或者一个平面或者更高维的超平面，使得预测值与真实值之间的误差最小化。

损失函数，代价函数，目标函数
损失函数和代价函数是同一个东西，目标函数是一个与他们相关但更广的概念，对于目标函数来说在有约束条件下的最小化就是损失函数（loss function）

有多个不同的函数来进行拟合，为了表示我们拟合的好坏，我们就用一个函数来度量拟合的程度，比如：

L(Y,f(X)) = (Y-f(X))^2 ，这个函数就称为损失函数(loss function)，或者叫代价函数(cost function)。损失函数越小，就代表模型拟合的越好。


优化方法：
1. 梯度下降法（Gradient Descent）

梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢

2. 牛顿法和拟牛顿法（Newton's method & Quasi-Newton Methods）

　　1）牛顿法（Newton's method）

　　牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。

2）拟牛顿法（Quasi-Newton Methods）

　　拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。


机器学习|线性回归三大评价指标实现『MAE, MSE, MAPE』


sklearn参数详解(不懂，纯抄)
　　1.sklearn参数详解：

　　    a.KNN

　　        •n_neighbors：默认为5，就是k-NN的k的值，选取最近的k个点。

　　        •weights：默认是uniform，参数可以是uniform、distance，也可以是用户自己定义的函数。uniform是均等的权重，就说所有的邻近点的权重都是相等的。distance是不均等的权重，距离近的点比距离远的点的影响大。用户自定义的函数，接收距离的数组，返回一组维数相同的权重。

　　        •algorithm：快速k近邻搜索算法，默认参数为auto，可以理解为算法自己决定合适的搜索算法。除此之外，用户也可以自己指定搜索算法ball_tree、kd_tree、brute方法进行搜索，brute是蛮力搜索，也就是线性扫描，当训练集很大时，计算非常耗时。kd_tree，构造kd树存储数据以便对其进行快速检索的树形数据结构，kd树也就是数据结构中的二叉树。以中值切分构造的树，每个结点是一个超矩形，在维数小于20时效率高。balltree是为了克服kd树高纬失效而发明的，其构造过程是以质心C和半径r分割样本空间，每个节点是一个超球体。

　　        •leaf_size：默认是30，这个是构造的kd树和ball树的大小。这个值的设置会影响树构建的速度和搜索速度，同样也影响着存储树所需的内存大小。需要根据问题的性质选择最优的大小。

　　        •metric：用于距离度量，默认度量是minkowski，也就是p=2的欧氏距离(欧几里德度量)。

　　        •p：距离度量公式。在上小结，我们使用欧氏距离公式进行距离度量。除此之外，还有其他的度量方法，例如曼哈顿距离。这个参数默认为2，也就是默认使用欧式距离公式进行距离度量。也可以设置为1，使用曼哈顿距离公式进行距离度量。

　　        •metric_params：距离公式的其他关键参数，这个可以不管，使用默认的None即可。

　　        •n_jobs：并行处理设置。默认为1，临近点搜索并行工作数。如果为-1，那么CPU的所有cores都用于并行工作。

　　    b.Kmeans

　　        •n_clusters:簇的个数，即你想聚成几类

　　        •init:初始簇中心的获取方法

　　        •n_init:获取初始簇中心的更迭次数，为了弥补初始质心的影响，算法默认会初始10次质心，实现算法，然后返回最好的结果。

　　        •max_iter:最大迭代次数（因为kmeans算法的实现需要迭代）

　　        •tol:容忍度，即kmeans运行准则收敛的条件

　　        •precompute_distances：是否需要提前计算距离，这个参数会在空间和时间之间做权衡，如果是True会把整个距离矩阵都放到内存中，auto会默认在数据样本大于featurs*samples的数量大于12e6的时候False,False时核心实现的方法是利用Cpython来实现的

　　        •verbose:冗长模式（不太懂是啥意思，反正一般不去改默认值）

　　        •random_state:随机生成簇中心的状态条件。

　　        •copy_x:对是否修改数据的一个标记，如果True，即复制了就不会修改数据。bool在scikit-learn很多接口中都会有这个参数的，就是是否对输入数据继续copy操作，以便不修改用户的输入数据。这个要理解Python的内存机制才会比较清楚。

　　        •n_jobs:并行设置

　　        •algorithm:kmeans的实现算法，有：’auto’,‘full’,‘elkan’,其中‘full’表示用EM方式实现

　　    c.朴素贝叶斯

　　        i.高斯朴素贝叶斯：

　　        •priors:先验概率大小，如果没有给定，模型则根据样本数据自己计算（利用极大似然法）。

　　        对象

　　        •class_prior_:每个样本的概率

　　        •class_count:每个类别的样本数量

　　        •theta_:每个类别中每个特征的均值

　　        •sigma_:每个类别中每个特征的方差

　　        ii.伯努利朴素贝叶斯

　　        •alpha:平滑因子，与多项式中的alpha一致。

　　        •binarize:样本特征二值化的阈值，默认是0。如果不输入，则模型会认为所有特征都已经是二值化形式了；如果输入具体的值，则模型会把大于该值的部分归为一类，小于的归为另一类。

　　        •fit_prior:是否去学习类的先验概率，默认是True

　　        •class_prior:各个类别的先验概率，如果没有指定，则模型会根据数据自动学习，每个类别的先验概率相同，等于类标记总个数N分之一。

　　    d.决策树：

　　        •criterion:特征选择的标准，有信息增益和基尼系数两种，使用信息增益的是ID3和C4.5算法（使用信息增益比），使用基尼系数的CART算法，默认是gini系数。

　　        •splitter:特征切分点选择标准，决策树是递归地选择最优切分点，spliter是用来指明在哪个集合上来递归，有“best”和“random”两种参数可以选择，best表示在所有特征上递归，适用于数据集较小的时候，random表示随机选择一部分特征进行递归，适用于数据集较大的时候。

　　        •max_depth:决策树最大深度，决策树模型先对所有数据集进行切分，再在子数据集上继续循环这个切分过程，max_depth可以理解成用来限制这个循环次数。

　　        •min_samples_split:子数据集再切分需要的最小样本量，默认是2，如果子数据样本量小于2时，则不再进行下一步切分。如果数据量较小，使用默认值就可，如果数据量较大，为降低计算量，应该把这个值增大，即限制子数据集的切分次数。

　　        •min_samples_leaf:叶节点（子数据集）最小样本数，如果子数据集中的样本数小于这个值，那么该叶节点和其兄弟节点都会被剪枝（去掉），该值默认为1。

　　        •min_weight_fraction_leaf:在叶节点处的所有输入样本权重总和的最小加权分数，如果不输入则表示所有的叶节点的权重是一致的。

　　        •max_features:特征切分时考虑的最大特征数量，默认是对所有特征进行切分，也可以传入int类型的值，表示具体的特征个数；也可以是浮点数，则表示特征个数的百分比；还可以是sqrt,表示总特征数的平方根；也可以是log2，表示总特征数的log个特征。

　　        •random_state:随机种子的设置，与LR中参数一致。

　　        •max_leaf_nodes:最大叶节点个数，即数据集切分成子数据集的最大个数。

　　        •min_impurity_decrease:切分点不纯度最小减少程度，如果某个结点的不纯度减少小于这个值，那么该切分点就会被移除。

　　        •min_impurity_split:切分点最小不纯度，用来限制数据集的继续切分（决策树的生成），如果某个节点的不纯度（可以理解为分类错误率）小于这个阈值，那么该点的数据将不再进行切分。

　　        •class_weight:权重设置，主要是用于处理不平衡样本，与LR模型中的参数一致，可以自定义类别权重，也可以直接使用balanced参数值进行不平衡样本处理。

　　        •presort:是否进行预排序，默认是False，所谓预排序就是提前对特征进行排序，我们知道，决策树分割数据集的依据是，优先按照信息增益/基尼系数大的特征来进行分割的，涉及的大小就需要比较，如果不进行预排序，则会在每次分割的时候需要重新把所有特征进行计算比较一次，如果进行了预排序以后，则每次分割的时候，只需要拿排名靠前的特征就可以了。

　　        对象/属性

　　        §classes_:分类模型的类别，以字典的形式输出

　　        §feature_importances_:特征重要性，以列表的形式输出每个特征的重要性max_features_:最大特征数

　　        §n_classes_:类别数，与classes_对应，classes_输出具体的类别

　　        §n_features_:特征数，当数据量小时，一般max_features和n_features_相等

　　        §n_outputs_:输出结果数tree_:输出整个决策树,用于生成决策树的可视化

　　        方法

　　        §decision_path(X):返回X的决策路径fit(X,y):在数据集(X,y)上使用决策树模型

　　        §get_params([deep]):获取模型的参数

　　        §predict(X):预测数据值X的标签

　　        §predict_log_proba(X):返回每个类别的概率值的对数

　　        §predict_proba(X):返回每个类别的概率值（有几类就返回几列值）

　　score(X,y):返回给定测试集和对应标签的平均准确率

